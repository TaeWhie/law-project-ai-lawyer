# Streamlit App
import streamlit as st
import os
import sys
import json
import time

# Add project root to sys.path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from rag.retriever import LawRetriever
from dotenv import load_dotenv

# Load env - Load this before any other logic
load_dotenv()

# --- Streamlit Cloud Compatibility ---
# LangChain needs os.environ['OPENAI_API_KEY'], but Streamlit Cloud stores it in st.secrets.
# We manually bridge them if needed.
if "OPENAI_API_KEY" in st.secrets:
    os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]

# --- Page Config (Must be first) ---
st.set_page_config(
    page_title="AI ë²•ë¥  ì¡°í•­ ì¶”ì²œê¸°",
    layout="wide",  # Use full width
    initial_sidebar_state="expanded"
)

# --- Bypass Streamlit's Email Prompt ---
if "user_email" not in st.session_state:
    st.session_state.user_email = "test@example.com"

def main():
    st.title("âš–ï¸ AI ë²•ë¥  ì¡°í•­ ì¶”ì²œê¸°")
    st.markdown("---")

    # --- Initialize Retriever (Cached) ---
    if "retriever" not in st.session_state:
        try:
            st.session_state.retriever = LawRetriever(
                persist_directory="data/chroma",
                collection_name="statutes"
            )
        except Exception as e:
            st.error(f"Retriever ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return

    retriever = st.session_state.retriever

    # --- Global 2-Column Layout ---
    # Left: Input & Articles
    # Right: AI Analysis
    # We use a [1, 1] split to give equal real estate, or [4, 5] for slightly wider analysis
    left_col, right_col = st.columns([1, 1], gap="large")

    with left_col:
        st.subheader("1. ìƒí™© ì„¤ëª… ë° ì¡°í•­ ê²€ìƒ‰")
        user_input = st.text_area(
            "ë²•ì  ìƒí™©ì„ ìì„¸íˆ ë¬˜ì‚¬í•´ ì£¼ì„¸ìš”:", 
            height=200,
            placeholder="ì˜ˆ: ì›”ê¸‰ì„ 3ë‹¬ì§¸ ëª» ë°›ê³  ìˆëŠ”ë° íšŒì‚¬ê°€ ë§í•  ê²ƒ ê°™ì•„ìš”."
        )
        
        search_clicked = st.button("ğŸ” ê´€ë ¨ ë²•ì•ˆ ì°¾ê¸°", use_container_width=True)
        
        st.markdown("### ğŸ“œ ê´€ë ¨ ë²•ë¥  ì¡°í•­")
        # Placeholder for articles
        article_container = st.container()

    with right_col:
        st.subheader("2. ğŸ‘¨â€âš–ï¸ AI ë³€í˜¸ì‚¬ ë¶„ì„")
        # Placeholder for analysis
        analysis_container = st.empty()
        analysis_container.info("ğŸ‘ˆ ì¢Œì¸¡ì—ì„œ ìƒí™©ì„ ì…ë ¥í•˜ê³  ê²€ìƒ‰í•˜ë©´, ë²•ë¥  ì „ë¬¸ê°€ì˜ ë¶„ì„ ê²°ê³¼ê°€ ì´ê³³ì— í‘œì‹œë©ë‹ˆë‹¤.")

    # --- Logic ---
    if search_clicked:
        if not user_input.strip():
            st.warning("ìƒí™©ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
            return
        
        with st.spinner("ë²•ë¥  ì¡°í•­ì„ ê²€ìƒ‰í•˜ê³  ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
            try:
                # 1. Retrieve
                # Increased k=5 to cover multiple issues (e.g., Wages + Dismissal)
                results = retriever.retrieve(user_input, k=5, use_llm_rerank=True)
                
                # 2. Display Articles (Left)
                with article_container:
                    if not results:
                        st.error("ê´€ë ¨ëœ ë²•ë¥  ì¡°í•­ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                    else:
                        st.success(f"ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ {len(results)}ê°œì˜ ì¡°í•­ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
                        for i, doc in enumerate(results):
                            meta = doc.metadata
                            title = meta.get("Title", "ë²•ë¥ ")
                            article_full = meta.get("Article", "ì¡°í•­")
                            
                            with st.expander(f"{i+1}. {title} > {article_full}", expanded=True):
                                st.markdown(f"**{article_full}**")
                                st.code(doc.page_content, language="text")
                
                # 3. Generate Analysis (Right)
                if results:
                    with analysis_container:
                        try:
                            from app.llm_factory import LLMFactory
                            from langchain_core.prompts import ChatPromptTemplate
                            
                            docs_context = ""
                            for doc in results:
                                docs_context += f"- {doc.metadata.get('Article', '')}: {doc.page_content}\n"
                            
                            lawyer_prompt = ChatPromptTemplate.from_template("""
                            ë„ˆëŠ” 20ë…„ ê²½ë ¥ì˜ ë”°ëœ»í•˜ê³  ìœ ëŠ¥í•œ ë…¸ë™ë²• ì „ë¬¸ ë³€í˜¸ì‚¬ë‹¤. ì˜ë¢°ì¸ì˜ [ìƒí™©]ê³¼ [ê´€ë ¨ ì¡°í•­]ì„ ë°”íƒ•ìœ¼ë¡œ ì•„ë˜ í˜•ì‹ì— ë§ì¶° ìƒë‹´ ë‚´ìš©ì„ ì‘ì„±í•˜ë¼.

                            [ì˜ë¢°ì¸ ìƒí™©]
                            {user_input}

                            [ê´€ë ¨ ë²•ë¥  ì¡°í•­]
                            {docs_context}

                            [ì‘ì„± í˜•ì¹™ (ë°˜ë“œì‹œ ì¤€ìˆ˜)]
                            1. **ğŸ’• ë”°ëœ»í•œ ìœ„ë¡œ**: ì˜ë¢°ì¸ì˜ í˜ë“  ìƒí™©ì— ëŒ€í•´ ì§„ì‹¬ ì–´ë¦° ê³µê°ê³¼ ìœ„ë¡œì˜ ë§ì„ ê±´ë„¤ë¼. (1-2ë¬¸ì¥)
                            2. **âš–ï¸ ë²•ë¥  ìš”ì•½ (ìŸì ë³„ êµ¬ë¶„)**: ìƒí™©ì— ë³µí•©ì ì¸ ë¬¸ì œ(ì˜ˆ: ë¶€ë‹¹í•´ê³  + ì„ê¸ˆì²´ë¶ˆ)ê°€ ìˆë‹¤ë©´, **1. ë¶€ë‹¹í•´ê³ , 2. ì„ê¸ˆì²´ë¶ˆ** ê³¼ ê°™ì´ ë²ˆí˜¸ë¥¼ ë§¤ê²¨ ê°ê° ëª…í™•íˆ ì§„ë‹¨í•˜ë¼.
                            3. **ğŸ›¡ï¸ ì¡°ì–¸ ë° ëŒ€ì²˜**: ê° ìŸì ë³„ë¡œ ì˜ë¢°ì¸ì´ ë‹¹ì¥ ì·¨í•´ì•¼ í•  í–‰ë™(ì¦ê±° í™•ë³´, ì‹ ê³  ì ˆì°¨ ë“±)ì„ êµ¬ì²´ì ìœ¼ë¡œ ì•ˆë‚´í•˜ë¼.
                            4. **âœ… ìŠ¤ìŠ¤ë¡œ ì²´í¬í•˜ê¸°**: ìŠ¹ì†Œ ê°€ëŠ¥ì„±ì„ íŒë‹¨í•˜ê¸° ìœ„í•œ í•µì‹¬ ì§ˆë¬¸ 5ê°€ì§€ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë‚˜ì—´í•˜ë¼.
                            5. **ğŸ“œ ê·¼ê±° ë²•ë ¹**: ìœ„ ìƒë‹´ì˜ ê·¼ê±°ê°€ ë˜ëŠ” ë²•ë¥  ì¡°í•­ ë²ˆí˜¸ì™€ ëª…ì¹­ì„ ëª…ì‹œí•˜ë¼.

                            [ì–´ì¡°]
                            ì „ë¬¸ì ì´ì§€ë§Œ, ì˜ë¢°ì¸ì„ ê°€ì¡±ì²˜ëŸ¼ ê±±ì •í•˜ëŠ” ë”°ëœ»í•˜ê³  ì •ì¤‘í•œ ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ë¼.
                            """)
                            
                            llm = LLMFactory.create_llm("openai", temperature=0.3)
                            chain = lawyer_prompt | llm
                            
                            # Streaming the response
                            message_placeholder = st.empty()
                            full_response = ""
                            
                            # Use chain.stream for token-by-token update
                            for chunk in chain.stream({
                                "user_input": user_input,
                                "docs_context": docs_context
                            }):
                                if chunk.content:
                                    full_response += chunk.content
                                    message_placeholder.markdown(full_response + "â–Œ")
                            
                            # Final update without cursor
                            message_placeholder.markdown(full_response)
                            
                        except Exception as e:
                            st.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

            except Exception as e:
                st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

if __name__ == "__main__":
    main()
